{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204193c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install -q peft transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cea11fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1c9ce79d1b4dfb80e6bac344c56f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d1e5e6dcf404f0d8b12c739c60a4e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9345bfcced684e2e8a0955d7b8ec4354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/490M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464713a14152410ca6a6476aa4233ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/464M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6825cbc71bcc4c518c78c8d9ee381b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/472M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c444f06ee34e1ba2c0c803f76141d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/464M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5255d29ea3834300803a37dca4eed992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/475M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f53ad43e6b4bc9993a6c8397500997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/470M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5073e7009c2450884a16845849899e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/478M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e1ad09c0d314b1e8ad0c9c3c93d39e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/486M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6eab98b2f5645a9bfd66cfd27d5235d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/423M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ec860d7bb14726ad30261dd819e7f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/413M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7deb2fc50547ba8cbd9357cfc941ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/426M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0953ba18a751409c8313ae220789694f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54783e1a49094d0fa6874b99d15ae233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/75750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37bc2954ee4b4f94b53b263dcdc645a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/25250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"food101\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b45a3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 75750\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 25250\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14aa1567",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ds[\"train\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e428a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beet_salad'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a494aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b46cb8ead5b9479198495816e3c938c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Owner\\.cache\\huggingface\\hub\\models--google--vit-base-patch16-224-in21k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0850df23bc71496c8a3837fe7f4cd268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "339c3427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose, # Compose a sequence of image transformations\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip, # Randomly flips image with probability 50%\n",
    "    RandomResizedCrop, # Radomly crops image. Augments training data\n",
    "    Resize,\n",
    "    ToTensor\n",
    ")\n",
    "\n",
    "normalize = Normalize(mean = image_processor.image_mean, std = image_processor.image_std)\n",
    "\n",
    "# This list of transfomrations will be applied to each image in dataset\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        RandomResizedCrop(image_processor.size[\"height\"]),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        Resize(image_processor.size[\"height\"]),\n",
    "        CenterCrop(image_processor.size[\"height\"]),\n",
    "        ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "\n",
    "def preprocess_train(example_batch):\n",
    "    example_batch[\"pixel_values\"] = [train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd00fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ds[\"train\"]\n",
    "val_ds = ds[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acb060c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label'],\n",
       "    num_rows: 75750\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f365f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_train and val are not called until\n",
    "train_ds.set_transform(preprocess_train)\n",
    "val_ds.set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d95f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you’ll need a data collator to create a batch of training and evaluation data and convert the labels to torch.tensor objects.\n",
    "import torch\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0ca8954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': [<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=384x512>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x512>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x383>],\n",
       " 'label': [6, 6, 6],\n",
       " 'pixel_values': [tensor([[[-1.0000, -1.0000, -0.9843,  ..., -0.6235, -0.6157, -0.6314],\n",
       "           [-1.0000, -1.0000, -1.0000,  ..., -0.6000, -0.6235, -0.6314],\n",
       "           [-1.0000, -0.9922, -0.9922,  ..., -0.6157, -0.6314, -0.6471],\n",
       "           ...,\n",
       "           [ 0.0275,  0.0745,  0.0980,  ..., -0.1137, -0.1294, -0.1451],\n",
       "           [ 0.0824,  0.0745,  0.0588,  ..., -0.1765, -0.1137, -0.0980],\n",
       "           [ 0.0588,  0.0980,  0.0980,  ..., -0.1529, -0.0824, -0.0902]],\n",
       "  \n",
       "          [[-0.7255, -0.7176, -0.7020,  ..., -0.5529, -0.5451, -0.5765],\n",
       "           [-0.7255, -0.7176, -0.7020,  ..., -0.5294, -0.5529, -0.5686],\n",
       "           [-0.7333, -0.7176, -0.7020,  ..., -0.5451, -0.5686, -0.5843],\n",
       "           ...,\n",
       "           [-0.0275,  0.0196,  0.0431,  ..., -0.1451, -0.1765, -0.1922],\n",
       "           [ 0.0196,  0.0118, -0.0039,  ..., -0.2078, -0.1451, -0.1373],\n",
       "           [-0.0196,  0.0196,  0.0196,  ..., -0.1843, -0.1137, -0.1216]],\n",
       "  \n",
       "          [[-0.7569, -0.7490, -0.7333,  ..., -0.4824, -0.4745, -0.4980],\n",
       "           [-0.7569, -0.7490, -0.7333,  ..., -0.4588, -0.4824, -0.4902],\n",
       "           [-0.7569, -0.7412, -0.7255,  ..., -0.4902, -0.5137, -0.5216],\n",
       "           ...,\n",
       "           [-0.2471, -0.2000, -0.1765,  ..., -0.3569, -0.3804, -0.3961],\n",
       "           [-0.2000, -0.2078, -0.2157,  ..., -0.4196, -0.3569, -0.3412],\n",
       "           [-0.2314, -0.1922, -0.1922,  ..., -0.3961, -0.3255, -0.3255]]]),\n",
       "  tensor([[[-0.2000, -0.1529, -0.1137,  ...,  0.4196,  0.4196,  0.4118],\n",
       "           [-0.0745, -0.0275,  0.0039,  ...,  0.4039,  0.3961,  0.3882],\n",
       "           [ 0.0275,  0.0667,  0.0902,  ...,  0.4039,  0.3961,  0.4039],\n",
       "           ...,\n",
       "           [ 0.6863,  0.6471,  0.6078,  ...,  0.1216,  0.2784,  0.3333],\n",
       "           [ 0.6784,  0.6627,  0.6235,  ...,  0.2157,  0.3569,  0.3804],\n",
       "           [ 0.6706,  0.6549,  0.6314,  ...,  0.3098,  0.4275,  0.4353]],\n",
       "  \n",
       "          [[-0.4745, -0.4275, -0.3882,  ..., -0.0980, -0.0980, -0.1059],\n",
       "           [-0.3490, -0.3020, -0.2706,  ..., -0.1137, -0.1216, -0.1294],\n",
       "           [-0.2314, -0.1843, -0.1686,  ..., -0.1137, -0.1216, -0.1216],\n",
       "           ...,\n",
       "           [ 0.5608,  0.5137,  0.4196,  ..., -0.1922, -0.0118,  0.0431],\n",
       "           [ 0.5765,  0.5294,  0.4667,  ..., -0.0824,  0.0745,  0.0902],\n",
       "           [ 0.5765,  0.5529,  0.5059,  ...,  0.0275,  0.1608,  0.1608]],\n",
       "  \n",
       "          [[-0.4431, -0.3961, -0.3412,  ..., -0.2235, -0.2235, -0.2314],\n",
       "           [-0.3176, -0.2706, -0.2314,  ..., -0.2392, -0.2471, -0.2549],\n",
       "           [-0.2000, -0.1608, -0.1294,  ..., -0.2392, -0.2471, -0.2549],\n",
       "           ...,\n",
       "           [ 0.6392,  0.5608,  0.4588,  ..., -0.1059,  0.0980,  0.1608],\n",
       "           [ 0.6549,  0.6000,  0.5137,  ...,  0.0196,  0.2078,  0.2392],\n",
       "           [ 0.6706,  0.6235,  0.5686,  ...,  0.1451,  0.2941,  0.3333]]]),\n",
       "  tensor([[[ 0.0196, -0.0118, -0.0353,  ..., -0.7804, -0.7725, -0.7647],\n",
       "           [ 0.0275,  0.0039, -0.0118,  ..., -0.7961, -0.7804, -0.7804],\n",
       "           [ 0.0353,  0.0353,  0.0353,  ..., -0.7882, -0.7804, -0.7804],\n",
       "           ...,\n",
       "           [ 0.2157,  0.2471,  0.2627,  ..., -0.4039, -0.3804, -0.3882],\n",
       "           [ 0.2549,  0.2784,  0.2706,  ..., -0.3725, -0.3804, -0.3961],\n",
       "           [ 0.2549,  0.2706,  0.2706,  ..., -0.3647, -0.3882, -0.3725]],\n",
       "  \n",
       "          [[-0.0118, -0.0510, -0.0824,  ..., -0.8588, -0.8510, -0.8431],\n",
       "           [-0.0118, -0.0431, -0.0667,  ..., -0.8745, -0.8588, -0.8588],\n",
       "           [-0.0196, -0.0353, -0.0431,  ..., -0.8667, -0.8588, -0.8588],\n",
       "           ...,\n",
       "           [ 0.2549,  0.2863,  0.3020,  ..., -0.4588, -0.4353, -0.4431],\n",
       "           [ 0.2941,  0.3176,  0.3098,  ..., -0.4275, -0.4353, -0.4510],\n",
       "           [ 0.2941,  0.3098,  0.3098,  ..., -0.4196, -0.4431, -0.4275]],\n",
       "  \n",
       "          [[-0.0431, -0.0824, -0.1216,  ..., -0.9294, -0.9216, -0.9137],\n",
       "           [-0.0196, -0.0510, -0.0902,  ..., -0.9451, -0.9294, -0.9294],\n",
       "           [-0.0118, -0.0353, -0.0510,  ..., -0.9373, -0.9294, -0.9294],\n",
       "           ...,\n",
       "           [ 0.2078,  0.2471,  0.2549,  ..., -0.5216, -0.5059, -0.5137],\n",
       "           [ 0.2549,  0.2863,  0.2784,  ..., -0.5059, -0.5137, -0.5294],\n",
       "           [ 0.2627,  0.2784,  0.2784,  ..., -0.4980, -0.5216, -0.5059]]])]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[:3] # We see preprocess function is applied on the fly when we access the train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89f7f3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3ad81b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 667,493 || all params: 86,543,818 || trainable%: 0.7713\n"
     ]
    }
   ],
   "source": [
    "# Every PEFT method requires a configuration that holds all the parameters specifying how the PEFT method should be applied. \n",
    "# Once the configuration is setup, pass it to the get_peft_model() function along with the base model to create a trainable PeftModel.\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    modules_to_save=[\"classifier\"],\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d68f598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "peft_model_id = f\"google/vit-base-patch16-224-in21k-lora\"\n",
    "batch_size = 128\n",
    "\n",
    "args = TrainingArguments(\n",
    "    peft_model_id,\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    fp16=True,\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    label_names=[\"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475c7c4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 20/740 00:47 < 31:21, 0.38 it/s, Epoch 0.13/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=image_processor,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6c9c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
