{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a84b307c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers[torch]\n",
    "!pip install trl\n",
    "!pip install evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21b7f74f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.44.2)\n",
      "Requirement already satisfied: accelerate in /opt/anaconda3/lib/python3.12/site-packages (0.34.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.25.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate) (2.4.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (69.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install --upgrade transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b8f1496",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from dataclasses import dataclass, field\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, TaskType, get_peft_model,PeftModelForSequenceClassification\n",
    "from transformers.utils import PaddingStrategy\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4e31bd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install datasets==2.16.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce1b3e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e808791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "252f9832062243c2ad0e2612c7f51a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#dataset = load_dataset('lvwerra/stack-exchange-paired',  data_dir='/Users/anantvirsingh/Desktop/huggingface_datasets/rlhf_stackexchange_paired/', verification_mode=\"no_checks\")\n",
    "dataset = load_dataset(\"lvwerra/stack-exchange-paired\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93d44d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['qid', 'question', 'date', 'metadata', 'response_j', 'response_k'],\n",
       "        num_rows: 26801833\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['qid', 'question', 'date', 'metadata', 'response_j', 'response_k'],\n",
       "        num_rows: 4483004\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "652efb6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qid': [6073935, 6073935, 6073935],\n",
       " 'question': [\"I have installed the Java 3D API on PC via the exe installer, which simply created a new directory with `j3dcore.jar`, `vecmath.jar`, `j3dutils.jar` in a lib sub-directory and `j3dcore-ogl.dll` in a bin sub-directory.\\n\\nNetbeans had no issues and my code compiled and executed smoothly, however once I built my project and tried to run it from the command prompt I got an `UnsatisfiedLinkError` saying that `no j3dcore-ogl in java.library.path`. \\n\\nGoogle came to the rescue and gave me 3 viable solutions:\\n\\n* by copying the dll file into my JRE's bin directory\\n* by adding the path of the dll file to the library path (`java -Djava.library.path=dllpath`)\\n* load the dll in the program with `System.load()` (I couldn't get this one to work, actually)\\n\\nMy question is: Is there an elegant solution to this problem, that I missed? \\n\\nIt seems tedious that for each different PC someone would like to use this program on, he'd have to either copy the dll or add it to the library path before it can run. (Side question: How come Netbeans didn't have a problem with the dll?)\",\n",
       "  \"I have installed the Java 3D API on PC via the exe installer, which simply created a new directory with `j3dcore.jar`, `vecmath.jar`, `j3dutils.jar` in a lib sub-directory and `j3dcore-ogl.dll` in a bin sub-directory.\\n\\nNetbeans had no issues and my code compiled and executed smoothly, however once I built my project and tried to run it from the command prompt I got an `UnsatisfiedLinkError` saying that `no j3dcore-ogl in java.library.path`. \\n\\nGoogle came to the rescue and gave me 3 viable solutions:\\n\\n* by copying the dll file into my JRE's bin directory\\n* by adding the path of the dll file to the library path (`java -Djava.library.path=dllpath`)\\n* load the dll in the program with `System.load()` (I couldn't get this one to work, actually)\\n\\nMy question is: Is there an elegant solution to this problem, that I missed? \\n\\nIt seems tedious that for each different PC someone would like to use this program on, he'd have to either copy the dll or add it to the library path before it can run. (Side question: How come Netbeans didn't have a problem with the dll?)\",\n",
       "  \"I have installed the Java 3D API on PC via the exe installer, which simply created a new directory with `j3dcore.jar`, `vecmath.jar`, `j3dutils.jar` in a lib sub-directory and `j3dcore-ogl.dll` in a bin sub-directory.\\n\\nNetbeans had no issues and my code compiled and executed smoothly, however once I built my project and tried to run it from the command prompt I got an `UnsatisfiedLinkError` saying that `no j3dcore-ogl in java.library.path`. \\n\\nGoogle came to the rescue and gave me 3 viable solutions:\\n\\n* by copying the dll file into my JRE's bin directory\\n* by adding the path of the dll file to the library path (`java -Djava.library.path=dllpath`)\\n* load the dll in the program with `System.load()` (I couldn't get this one to work, actually)\\n\\nMy question is: Is there an elegant solution to this problem, that I missed? \\n\\nIt seems tedious that for each different PC someone would like to use this program on, he'd have to either copy the dll or add it to the library path before it can run. (Side question: How come Netbeans didn't have a problem with the dll?)\"],\n",
       " 'date': ['2011/05/20', '2011/05/20', '2011/05/20'],\n",
       " 'metadata': [['https://Stackoverflow.com/questions/6073935',\n",
       "   'https://Stackoverflow.com',\n",
       "   'https://Stackoverflow.com/users/639422/'],\n",
       "  ['https://Stackoverflow.com/questions/6073935',\n",
       "   'https://Stackoverflow.com',\n",
       "   'https://Stackoverflow.com/users/639422/'],\n",
       "  ['https://Stackoverflow.com/questions/6073935',\n",
       "   'https://Stackoverflow.com',\n",
       "   'https://Stackoverflow.com/users/639422/']],\n",
       " 'response_j': [\"> \\n> Making my Java program easily distributable\\n> \\n> \\n> \\n\\nIf you mean 'easy for the end user' look to [Java Web Start](https://stackoverflow.com/tags/java-web-start/info).\\n\\n---\\n\\nA passer-by asks:\\n\\n> \\n> Can you package the dll dependencies with Web Start? \\n> \\n> \\n> \\n\\nYes, but much, much better. You can package the natives for each platform in separate Jars, and supply them only to the platform that uses that native, even so far as partitioning the download between 32 & 64 bit versions of the natives.\\n\\nJWS puts the natives on the run-time class-path of the application, ready for loading in code.\\n\\nThis all happens automatically for the end user, they click a link, approve the trust dialog(s) when asked, and the application installs - possibly with desktop integration, and appears on screen like magic.\\n\\nJWS apps. that use natives need to be distributed as `all-permissions` security level, because the JVM cannot guarantee the actions of anything that 'goes native'.\",\n",
       "  \"*Edit - After re-reading your question, your issue sounds different. However I'm able to get my running like so, by just dropping all dll files in the same directory as the .bat file starting the java process:*\\n\\n*java -classpath ./YourJar.jar;./lib/j3dcore.jar;./lib/vecmath.jar;./lib/j3dutils.jar package.MainClass*\\n\\n*And that works on multiple user's PCs, so I know simply dropping it in the working directory works.*\\n\\nI believe it depends on the version of Java being used - 64 bit or 32 bit. The correct dll file (of the same name) needs to be in the working directory.\\n\\nI think I was getting a similar problem when the wrong dll was being used, and it's not OS-dependent (if your 64 bit OS has 32-bit Java installed, you'd need the 32 bit j3dcore-ogl.dll file).\\n\\nSo the question is, which version of Java are you using *(when running outside of your IDE)*, and which version of the dll are you putting (if any) in the working directory? I don't need any dll files in my path settings to get this working on other's PCs, and did not use System.load(), and did NOT copy files into my user's JRE/bin directory - so I know this is possible without the 3 options you mention.\",\n",
       "  \"> \\n> Making my Java program easily distributable\\n> \\n> \\n> \\n\\nIf you mean 'easy for the end user' look to [Java Web Start](https://stackoverflow.com/tags/java-web-start/info).\\n\\n---\\n\\nA passer-by asks:\\n\\n> \\n> Can you package the dll dependencies with Web Start? \\n> \\n> \\n> \\n\\nYes, but much, much better. You can package the natives for each platform in separate Jars, and supply them only to the platform that uses that native, even so far as partitioning the download between 32 & 64 bit versions of the natives.\\n\\nJWS puts the natives on the run-time class-path of the application, ready for loading in code.\\n\\nThis all happens automatically for the end user, they click a link, approve the trust dialog(s) when asked, and the application installs - possibly with desktop integration, and appears on screen like magic.\\n\\nJWS apps. that use natives need to be distributed as `all-permissions` security level, because the JVM cannot guarantee the actions of anything that 'goes native'.\"],\n",
       " 'response_k': ['If you put the dlls in the same directory than you Jar, does it work?\\nIf yes, you could consider distributing it like this.',\n",
       "  'If you put the dlls in the same directory than you Jar, does it work?\\nIf yes, you could consider distributing it like this.',\n",
       "  'I guess DLL are searched in all folders in %PATH% on windows. (LD\\\\_LIBRARY\\\\_PATH for UNIX flavors)\\n\\nCould you try by adding the path to dll to %path% variable?\\n\\nIt appears that you are trying package a product with many jars as dependencies. You may benefit from [One-Jar](http://one-jar.sourceforge.net/index.php?page=details&file=native). It claims to have native dll support.']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4724ae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4161d4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_model_name = \"reward_model_peft_lora\"\n",
    "output_dir = \"/Users/anantvirsingh/Desktop/huggingface/transformers/output_models/\"\n",
    "learning_rate = \"1e-5\"\n",
    "train_epochs = 10\n",
    "reward_model_checkpoint = \"gpt2\"\n",
    "rl_model_checkpoint = \"gpt2\"\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f3ee833",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# HuggingFace Trainer API takes TrainingArguments as input (although optional). So lets set config for training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=3,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=10,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[],\n",
    "    bf16=True,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "539b6640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(reward_model_checkpoint, use_auth_token=os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f2a2b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PEFT config parameters for LoRA\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type = TaskType.SEQ_CLS, # based on task specified here, task_type helps set correct head & correct loss fn.\n",
    "    inference_mode = False,\n",
    "    r = 8, # Set rank for LoRA matrices\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ed9e9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Set reward model from checkpoint. num_labels = 1 as we want our reward model to output a scalar\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    reward_model_checkpoint, num_labels=1,  use_auth_token=os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "476a7396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 295,680 || all params: 124,736,256 || trainable%: 0.2370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/peft/tuners/lora/layer.py:1091: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Now we have defined LoRA config and we also have defined a reward_model, lets now feed both these int get_peft_model()\n",
    "# and get a PEFT model\n",
    "reward_model_peft = PeftModelForSequenceClassification(reward_model, peft_config)\n",
    "#reward_model_peft = get_peft_model(peft_config, reward_model)\n",
    "reward_model_peft.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cbbd241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to do this for gpt2, because it doesn't have an official pad token.\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "reward_model_peft.config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d72e507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input : input_data_batch i.e batch of Dataset :\n",
    "        Dataset({\n",
    "                features: ['qid', 'question', 'date', 'metadata', 'response_j', 'response_k'],\n",
    "                num_rows: 26801833\n",
    "            })\n",
    "For each question + response, we combine then together as \n",
    "(\n",
    "    Question : input_sample[\"question\"]\n",
    "    \n",
    "    Answer : input_sample[\"response\"]\n",
    ")\n",
    "and we create 2 columns\n",
    "\n",
    "Column 1 : (\n",
    "    Question : input_sample[\"question\"]\n",
    "    \n",
    "    Answer : input_sample[\"response_j\"] # This is the response ranked better by human annotators\n",
    ")\n",
    "Column 2 : (\n",
    "    Question : input_sample[\"question\"]\n",
    "    \n",
    "    Answer : input_sample[\"response_k\"] # This is the response ranked lower by human annotators\n",
    ")\n",
    "Each column will be a large string and hence will be tokenized. So the tokenizer will return 'input_ids' for that column. \n",
    "Along with that, tokenizer will also return attention_mask.\n",
    "\n",
    "So in total we will return 4 columns\n",
    "Column 1 : tokenized_j\n",
    "Column 2 : attention_mask_j\n",
    "Column 3 : tokenized_k\n",
    "Column 2 : attention_mask_k\n",
    "\"\"\"\n",
    "def preprocess_function(input_data_batch):\n",
    "    new_data_dict = {\n",
    "        \"input_ids_j\" : [],\n",
    "        \"attention_mask_j\" : [],\n",
    "        \"input_ids_k\" : [],\n",
    "        \"attention_mask_k\" : []\n",
    "    }\n",
    "    \n",
    "    # Iterate over all examples in batch\n",
    "    for question, response_j, response_k in zip(input_data_batch[\"question\"], input_data_batch[\"response_j\"], input_data_batch[\"response_k\"]):\n",
    "        tokenized_j = tokenizer(\" Question : \" + question + \"\\n \\n Answer : \" + response_j, truncation=True, max_length=512)\n",
    "        tokenized_k = tokenizer(\" Question : \" + question + \"\\n \\n Answer : \" + response_k, truncation=True, max_length=512)\n",
    "    \n",
    "        new_data_dict[\"input_ids_j\"].append(tokenized_j[\"input_ids\"])\n",
    "        new_data_dict[\"attention_mask_j\"].append(tokenized_j[\"attention_mask\"])\n",
    "        new_data_dict[\"input_ids_k\"].append(tokenized_k[\"input_ids\"])\n",
    "        new_data_dict[\"attention_mask_k\"].append(tokenized_k[\"attention_mask\"])\n",
    "    \n",
    "    # We just return new_data_dict, HF Datasets map() will take care of creating a new Dataset object with new_data_dict\n",
    "    # and keep appending to it. HG Datasets does a lot boilerplate stuff for us\n",
    "    return new_data_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c6d8d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we apply above preprocess function to each element/batch in our dataset\n",
    "final_train_dataset = dataset[\"train\"].select(range(1000)).map(\n",
    "    preprocess_function,\n",
    "    batched = True,\n",
    "    remove_columns = dataset[\"train\"].column_names # We just care about numbers(input_ids, attention_mask etc.), remove all text\n",
    ")\n",
    "\n",
    "final_eval_dataset = dataset[\"test\"].select(range(1000)).map(\n",
    "    preprocess_function,\n",
    "    batched = True,\n",
    "    remove_columns = dataset[\"test\"].column_names # We just care about numbers(input_ids, attention_mask etc.), remove all text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3737ffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output = reward_model_peft(input_ids = final_train_dataset[\"input_ids_chosen\"], attention_mask = final_train_dataset[\"attention_mask_chosen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57ea8c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids_j', 'attention_mask_j', 'input_ids_k', 'attention_mask_k'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96e9da21",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RewardDataCollatorWithPadding:\n",
    "    tokenizer: tokenizer\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        features_j = []\n",
    "        features_k = []\n",
    "        for feature in features:\n",
    "            features_j.append(\n",
    "                {\n",
    "                    \"input_ids\": feature[\"input_ids_j\"],\n",
    "                    \"attention_mask\": feature[\"attention_mask_j\"],\n",
    "                }\n",
    "            )\n",
    "            features_k.append(\n",
    "                {\n",
    "                    \"input_ids\": feature[\"input_ids_k\"],\n",
    "                    \"attention_mask\": feature[\"attention_mask_k\"],\n",
    "                }\n",
    "            )\n",
    "        batch_j = self.tokenizer.pad(\n",
    "            features_j,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "        batch_k = self.tokenizer.pad(\n",
    "            features_k,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "        batch = {\n",
    "            \"input_ids_j\": batch_j[\"input_ids\"],\n",
    "            \"attention_mask_j\": batch_j[\"attention_mask\"],\n",
    "            \"input_ids_k\": batch_k[\"input_ids\"],\n",
    "            \"attention_mask_k\": batch_k[\"attention_mask\"],\n",
    "            \"return_loss\": True,\n",
    "        }\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06e063a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metric that we'll use for validation.\n",
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, _ = eval_pred\n",
    "    # Here, predictions is rewards_j and rewards_k.\n",
    "    # We want to see how much of the time rewards_j > rewards_k.\n",
    "    predictions = np.argmax(predictions, axis=0)\n",
    "    labels = np.zeros(predictions.shape)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc49fa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we are using a custom pairwise loss as per : \n",
    "# 1. https://huyenchip.com/2023/05/02/rlhf.html#phase_3_rlhf\n",
    "# 2. https://huggingface.co/blog/stackllama\n",
    "# We override the compute_loss() function of Trainer class\n",
    "class RewardTrainer(Trainer):\n",
    "    def compute_loss(self, reward_model_peft, inputs, return_outputs = False):\n",
    "        \n",
    "        # Forward pass with batch j i.e chosen answer\n",
    "        rewards_for_chosen_answers = reward_model_peft(input_ids = inputs[\"input_ids_j\"], attention_mask = inputs[\"attention_mask_j\"])[0]\n",
    "        \n",
    "        # Forward pass with batch k i.e rejected answer\n",
    "        rewards_for_rejected_answers = reward_model_peft(input_ids = inputs[\"input_ids_k\"], attention_mask = inputs[\"attention_mask_k\"])[0]\n",
    "        \n",
    "        # Pairwise loss calculation for batch with dimenions x * y\n",
    "        loss = -nn.functional.logsigmoid(rewards_for_chosen_answers - rewards_for_rejected_answers).mean()\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f537bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can use the RewardTrainer class same way we can use Trainer class i.e pass TrainingConfig to Trainer and then train model \n",
    "\n",
    "trainer = RewardTrainer(\n",
    "    model = reward_model_peft,\n",
    "    args = training_args,\n",
    "    train_dataset=final_train_dataset,\n",
    "    eval_dataset=final_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=RewardDataCollatorWithPadding(tokenizer=tokenizer, max_length=256),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4249a8-625b-4977-a7db-f85288fd7f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2888: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='29' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 29/375 00:28 < 06:02, 0.95 it/s, Epoch 0.22/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2888: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2888: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f665a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
