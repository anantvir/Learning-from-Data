{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75043677-c489-4fb6-84cb-5b5143b48a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers[torch]\n",
    "!pip install trl\n",
    "!pip install evaluate\n",
    "!pip install transformers accelerate\n",
    "!pip install bitsandbytes==0.44.0\n",
    "!pip install accelerate --upgrade\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e13c3986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from dataclasses import dataclass, field\n",
    "from peft import LoraConfig, TaskType, get_peft_model,PeftModelForSequenceClassification\n",
    "from transformers.utils import PaddingStrategy\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7886c177",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reward_model_checkpoint = \"prajjwal1/bert-small\"\n",
    "reward_model_checkpoint = \"ibm-granite/granite-7b-base\"\n",
    "dataset_checkpoint = \"Anthropic/hh-rlhf\"\n",
    "output_dir = \"D:\\huggingface\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4bd926e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3d4ac68b7f40c48d641a8ae98fb0ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/5.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e392802b50ae4eb593b9ef63db4b6c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/13.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8acf66704d8403c9144dd5e4b7ec1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/16.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "818da406069d4b77899193eaf62f9a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/20.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a57a86c811408e96df0984a13213f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/25.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8150c5d56c244cb1b0a118aae9c07fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/743k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5bb4ad8ea914902bf78ffac3ee4a0b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/875k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da3f50cc9e04282bbcf8e03ce56262d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.05M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ebb1d41b8d425fbc0e9630858da370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8be887b0e9492990b3133f0bf1fea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ac970085724953b43d5711b2bc2055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset\n",
    "\n",
    "train_dataset = load_dataset(dataset_checkpoint, split=\"train\")\n",
    "test_dataset = load_dataset(dataset_checkpoint, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38d32386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>\n"
     ]
    }
   ],
   "source": [
    "# Define Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(reward_model_checkpoint, truncation=True)\n",
    "# Set the pad_token to be the same as eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec69d903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8556e4e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chosen', 'rejected'],\n",
       "    num_rows: 160800\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fd90bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chosen\n",
      "rejected\n"
     ]
    }
   ],
   "source": [
    "for feature in train_dataset.features:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da4b8a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "189a50b3e07f4c8f87d0d108bfa8eaf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/160800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb236418052846c1b7b07e796f8222cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8552 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" Preprocessing : tokenize the data\n",
    "\n",
    "Input -> 'chosen' , output ->  'input_ids_chosen' , 'attention_mask_chosen'\n",
    "Input -> 'rejected' , output ->  'input_ids_rejected' , 'attention_mask_rejected'\n",
    "\n",
    "So total 4 columns in output dataset and each column to be tokenized\n",
    "\n",
    "\n",
    "Returns : new HuggingFace Dataset object. map() function will create a new Dataset from the value we return from map().\n",
    "map() will be applied to each element of original dataset and since the same value is returned for every map() operation,\n",
    "it becomes easier to append it to the new Dataset object\n",
    "\n",
    "\"\"\"\n",
    "def preprocess_function(examples_batch):\n",
    "    new_dataset_schema = {\n",
    "        \"input_ids_chosen\" : [],\n",
    "        \"attention_mask_chosen\" : [],\n",
    "        \"input_ids_rejected\" : [],\n",
    "        \"attention_mask_rejected\" : []\n",
    "    }\n",
    "    \n",
    "    # Iterate over columns in Dataset\n",
    "    for chosen, rejected in zip(examples_batch[\"chosen\"], examples_batch[\"rejected\"]):\n",
    "        tokenized_chosen = tokenizer(chosen, max_length=512, truncation=True)\n",
    "        tokenized_rejected = tokenizer(rejected, max_length=512, truncation=True)\n",
    "\n",
    "        new_dataset_schema[\"input_ids_chosen\"].append(tokenized_chosen[\"input_ids\"])\n",
    "        new_dataset_schema[\"attention_mask_chosen\"].append(tokenized_chosen[\"attention_mask\"])\n",
    "        new_dataset_schema[\"input_ids_rejected\"].append(tokenized_rejected[\"input_ids\"])\n",
    "        new_dataset_schema[\"attention_mask_rejected\"].append(tokenized_rejected[\"attention_mask\"])\n",
    "    \n",
    "    return new_dataset_schema\n",
    "    \n",
    "final_tokenized_train_dataset = train_dataset.map(\n",
    "                                    preprocess_function,\n",
    "                                    batched = True,\n",
    "                                    remove_columns = train_dataset.column_names\n",
    "                                )\n",
    "\n",
    "final_tokenized_test_dataset = test_dataset.map(\n",
    "                                    preprocess_function,\n",
    "                                    batched = True,\n",
    "                                    remove_columns = test_dataset.column_names\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b96ec49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],\n",
       "    num_rows: 160800\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_tokenized_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d0c1973",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define DataCollator. This will pad the batch. Since accepted column needs to be passed to model separately in 1st pass along with attention_mask_accepted\n",
    "and rejected column needs to be passed in second pass along with attention_mask_rejected, so we separate out these two\n",
    "i.e we create a separate padding logic just like in previous reward model training.\n",
    "\n",
    "Input : Batch of examples generated from below Dataset (Unpadded) in a list of dictionaries of type List[Dict[str, Any]].\n",
    "This batch is generated by HuggingFace Trainer class using PyTorch DataLoader API\n",
    "\n",
    "[\n",
    "    {\n",
    "        \"input_ids_chosen\" :[],\n",
    "        \"attention_mask_chosen\" : [],\n",
    "        \"input_ids_rejected\" : [],\n",
    "        \"attention_mask_rejected\" : []\n",
    "    },\n",
    "    {\n",
    "        \"input_ids_chosen\" :[],\n",
    "        \"attention_mask_chosen\" : [],\n",
    "        \"input_ids_rejected\" : [],\n",
    "        \"attention_mask_rejected\" : []\n",
    "    }, \n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    . \n",
    "]\n",
    "\n",
    "[Dataset({\n",
    "    features: ['input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],\n",
    "    num_rows: 160800\n",
    "})] ---> unpadded \n",
    "\n",
    "Output : Padded batch\n",
    "\n",
    "Dataset({\n",
    "    features: ['input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],\n",
    "    num_rows: 160800\n",
    "}) ---> padded \n",
    "\n",
    "Since 'chosen' and 'rejected' columns are of different lengths, we need to pad their input_ids and attention_mask separately\n",
    "\n",
    "\"\"\"\n",
    "@dataclass\n",
    "class RewardDataCollatorWithPadding:\n",
    "    tokenizer: tokenizer\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        accepted = [] # we process accepted and rejected separately in 2 forward passes. So we pad them separately too to make process more efficient\n",
    "        rejected = []\n",
    "\n",
    "        for example in batch:\n",
    "            accepted.append(\n",
    "                {\n",
    "                    # Here we use \"input_ids\" as key because this batch will be passed to our tokenizers pad() method\n",
    "                    # and pad() only expects \"input_ids\" and \"attention_maks\" as keys\n",
    "                    \"input_ids\" : example[\"input_ids_chosen\"],\n",
    "                    \"attention_mask\" : example[\"attention_mask_chosen\"]\n",
    "                }\n",
    "            )\n",
    "            rejected.append(\n",
    "                {\n",
    "                    \"input_ids\" : example[\"input_ids_rejected\"],\n",
    "                    \"attention_mask\" : example[\"attention_mask_rejected\"]\n",
    "                }\n",
    "            )\n",
    "        padded_batch_accepeted = tokenizer.pad(\n",
    "                                accepted,\n",
    "                                padding=True,\n",
    "                                return_tensors=self.return_tensors,\n",
    "                                verbose=True)\n",
    "\n",
    "        padded_batch_rejected = tokenizer.pad(\n",
    "                                rejected,\n",
    "                                padding=True,\n",
    "                                return_tensors=self.return_tensors,\n",
    "                                verbose=True)\n",
    "        \n",
    "    \n",
    "        batch = {\n",
    "            \"input_ids_chosen\" : padded_batch_accepeted[\"input_ids\"],\n",
    "            \"attention_mask_chosen\" : padded_batch_accepeted[\"attention_mask\"],\n",
    "            \"input_ids_rejected\" : padded_batch_rejected[\"input_ids\"],\n",
    "            \"attention_mask_rejected\" : padded_batch_rejected[\"attention_mask\"],\n",
    "            \"return_loss\" : True\n",
    "        }\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c906f1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up quantization config for 4-bit\n",
    "# quant_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,     # Enables 4-bit quantization\n",
    "#     bnb_4bit_use_double_quant=True,  # Optional: Uses double quantization for further compression\n",
    "#     bnb_4bit_quant_type='nf4',  # Quantization type: 'nf4' recommended for LLaMA\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9268f163",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Since we need to fine tune an existing LLM for our reward model, LoRA would be a good choice here. We will just train\n",
    "the LoRA layers and keep rest of the model frozen\n",
    "\"\"\"\n",
    "\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    task_type = TaskType.SEQ_CLS, # based on task specified here, task_type helps set correct head & correct loss fn.\n",
    "    inference_mode = False,\n",
    "    r = 4, # Set rank for LoRA matrices\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49a81ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44.0\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "print(bnb.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fafc280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9dd7f1c5000485e844620a35c9ee8c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at ibm-granite/granite-7b-base and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Get base reward model from huggingface hub which we will tunr with LoRA\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(reward_model_checkpoint, num_labels = 1, load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eba44ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,101,248 || all params: 6,609,448,960 || trainable%: 0.0318\n"
     ]
    }
   ],
   "source": [
    "# We now have peft_config and base model. We can easily get a LoRA model through the method get_peft_model()\n",
    "\n",
    "peft_reward_model = get_peft_model(reward_model, peft_config)\n",
    "peft_reward_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f20ae3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define custom pairwise loss by overriding compute_loss function of Trainer class\n",
    "\n",
    "class RewardTrainer(Trainer):\n",
    "    \n",
    "    # Input -> a batch coming from our custom DataCollator \n",
    "    def compute_loss(self, peft_reward_model, inputs, return_outputs = False):\n",
    "        \n",
    "        # R_a reward for forward pass 1 with chosen inputs\n",
    "        rewards_for_chosen_answers = peft_reward_model(input_ids = inputs[\"input_ids_chosen\"], attention_mask = inputs[\"attention_mask_chosen\"])[0]\n",
    "        \n",
    "        # R_b reward for forward pass 2 with rejected inputs\n",
    "        rewards_for_rejected_answers = peft_reward_model(input_ids = inputs[\"input_ids_rejected\"], attention_mask = inputs[\"attention_mask_rejected\"])[0]\n",
    "        \n",
    "        # Pairwise loss calculation for batch with dimenions x * y\n",
    "        loss = -nn.functional.logsigmoid(rewards_for_chosen_answers - rewards_for_rejected_answers).mean()\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74289c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have our final dataset, data_collator, PEFT model, custom loss function defined. All we need to do it\n",
    "# create training arguments for our Trainer \n",
    "\n",
    "# HuggingFace Trainer API takes TrainingArguments as input (although optional). So lets set config for training\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    learning_rate=1e-6,\n",
    "    num_train_epochs=3,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[],\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    per_device_train_batch_size = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9542bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = RewardTrainer(\n",
    "            model = peft_reward_model,\n",
    "            args = training_args,\n",
    "            train_dataset=final_tokenized_train_dataset,\n",
    "            eval_dataset=final_tokenized_test_dataset,\n",
    "            data_collator=RewardDataCollatorWithPadding(tokenizer=tokenizer, max_length=128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f1f75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='482400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [     9/482400 06:54 < 7938:55:08, 0.02 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b98e3af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
